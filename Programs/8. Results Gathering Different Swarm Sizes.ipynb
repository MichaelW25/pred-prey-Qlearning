{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcc3fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import time\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d0945",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1660a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "Learning_Rate = 0.001\n",
    "Error_Rate = 0.9\n",
    "Epsilon = 0.2\n",
    "Episodes = 1000\n",
    "\n",
    "# Simulation Parameters\n",
    "planet_size = 12\n",
    "num_robots = 5\n",
    "Movement_Budget = (planet_size * planet_size) * 1.2   \n",
    "\n",
    "simulationLoops = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb4136",
   "metadata": {},
   "source": [
    "## Environments (Planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d43e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class Planet(object):\n",
    "    def __init__(self, planet_width, num_robots):\n",
    "        # start with defining your planet\n",
    "        self.planet_width = planet_width\n",
    "        self.planet_edge = planet_width - 1\n",
    "        self.planet = np.zeros((planet_width, planet_width)).astype(int)\n",
    "        \n",
    "        #Traps\n",
    "        \"\"\"\n",
    "        self.planet[2, 8] = 1\n",
    "        self.planet[4, 5] = 1\n",
    "        self.planet[4, 6] = 1\n",
    "        self.planet[4, 7] = 1\n",
    "        self.planet[5, 5] = 1\n",
    "        self.planet[5, 6] = 1\n",
    "        self.planet[5, 7] = 1\n",
    "        self.planet[9, 9] = 1\n",
    "        self.planet[9, 10] = 1\n",
    "        self.planet[10, 9] = 1\n",
    "        self.planet[10, 10] = 1\n",
    "        self.planet[7, 1] = 1\n",
    "        \"\"\"\n",
    "        #Robots\n",
    "        self.num_robots = num_robots\n",
    "        self.robot_positions = []\n",
    "        self.prev_robot_locations = np.zeros((num_robots, 4)).astype(int)\n",
    "        self.agent_setup()\n",
    "               \n",
    "        self.steps = 0 # contains num steps robots took\n",
    "        self.allowed_states = None # for now, this is none\n",
    "        self.construct_allowed_states() \n",
    "    \n",
    "    def agent_setup(self):\n",
    "        for r in range(self.num_robots):\n",
    "            found = False\n",
    "            val_r = r\n",
    "            while found == False:  \n",
    "                temp_y = val_r // self.planet_width\n",
    "                temp_x = val_r % self.planet_width\n",
    "                if self.planet[temp_y, temp_x] == 0:\n",
    "                    found = True\n",
    "                else:\n",
    "                    val_r += 1\n",
    "                    \n",
    "            self.robot_positions.append((temp_y, temp_x))\n",
    "            self.planet[temp_y, temp_x] = 2\n",
    "            self.prev_robot_locations[r][0] = temp_y\n",
    "            self.prev_robot_locations[r][1] = temp_x\n",
    "            self.prev_robot_locations[r][2] = temp_y\n",
    "            self.prev_robot_locations[r][3] = temp_x\n",
    "    \n",
    "    def is_allowed_move(self, state, action):\n",
    "        y, x = state\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        # moving off the board\n",
    "        if y < 0 or x < 0 or y > self.planet_edge or x > self.planet_edge:\n",
    "             return False\n",
    "        # moving into empty space\n",
    "        elif self.planet[y, x] != 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def construct_allowed_states(self):\n",
    "        allowed_states = {}\n",
    "        for y, row in enumerate(self.planet):\n",
    "            for x, col in enumerate(row):\n",
    "                # iterate through all valid spaces\n",
    "                if self.planet[(y,x)] != 1:\n",
    "                    allowed_states[(y,x)] = []\n",
    "                    for action in ACTIONS:\n",
    "                        if self.is_allowed_move((y, x), action):\n",
    "                            allowed_states[(y,x)].append(action)\n",
    "        self.allowed_states = allowed_states\n",
    "        \n",
    "    def update_planet(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        old_y, old_x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        self.planet[y, x] = 2\n",
    "        self.planet[old_y, old_x] = 0\n",
    "        self.robot_positions[robot_num] = (y, x)\n",
    "        \n",
    "    def invalid_move(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        return (y, x)  \n",
    "                \n",
    "    def is_game_over(fully_covered):\n",
    "        return fully_covered\n",
    "    \n",
    "    def get_distance(self, prey, pred):\n",
    "        prey_location = self.robot_positions[prey]\n",
    "        pred_location = self.robot_positions[pred]\n",
    "        distance = math.sqrt(math.pow((prey_location[0]-pred_location[0]), 2) + math.pow((prey_location[1]-pred_location[1]), 2))\n",
    "        return distance\n",
    "    \n",
    "    def neighbour_distance(self, prey):\n",
    "        min_dist = 1e10\n",
    "        max_dist = 0\n",
    "        location = self.robot_positions[prey]\n",
    "        action = 0\n",
    "        while action < 4:\n",
    "            temp_y, temp_x = location\n",
    "            temp_y += ACTIONS[action][0]\n",
    "            temp_x += ACTIONS[action][1]\n",
    "            if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                if not(planet.planet[temp_y, temp_x] == 2):\n",
    "                    for i in range(self.num_robots):\n",
    "                        if not (i == prey):\n",
    "                            pred_location = self.robot_positions[i]\n",
    "                            distance = distance = math.sqrt(math.pow((temp_y-pred_location[0]), 2) + math.pow((temp_x-pred_location[1]), 2))\n",
    "                            if (distance > max_dist):\n",
    "                                max_dist = distance\n",
    "                            if (distance < min_dist):\n",
    "                                decided_x = temp_x\n",
    "                                decided_y = temp_y\n",
    "                                decided_2_r = i\n",
    "                                min_dist = distance\n",
    "            action += 1\n",
    "        return min_dist, max_dist\n",
    "        \n",
    "    def give_reward(self, state_history, robot_num, visited_locations):\n",
    "        # Parameters \n",
    "        predator_weight = 2\n",
    "        smoothness_weight = 1\n",
    "        boundary_weight = 1.5\n",
    "        \n",
    "        #Updated Location - Location the reward is being calculated for\n",
    "        location = self.robot_positions[robot_num]\n",
    "        \n",
    "        # If new location get specialised reward\n",
    "        if location not in visited_locations:\n",
    "            \n",
    "            # Predator-Prey\n",
    "            # Distance between agents neighbours and the predator (other agents)\n",
    "            min_dist, max_dist = self.neighbour_distance(robot_num)\n",
    "            # Closest agent\n",
    "            current_dist = 1e10\n",
    "            for i in range(self.num_robots):\n",
    "                if not (i == robot_num):\n",
    "                    distance = self.get_distance(robot_num, i)\n",
    "                    if (distance < current_dist):\n",
    "                        decided_r = i\n",
    "                        current_dist = distance\n",
    "                        \n",
    "            predator_reward = (current_dist - min_dist) / (max_dist - min_dist)\n",
    "            \n",
    "            \n",
    "            # Smoothness - Reward Function   \n",
    "            # 2 prev - 1 prev - curr\n",
    "            prev_2_y = self.prev_robot_locations[robot_num][0]\n",
    "            prev_2_x = self.prev_robot_locations[robot_num][1]\n",
    "            prev_1_y = self.prev_robot_locations[robot_num][2]\n",
    "            prev_1_x = self.prev_robot_locations[robot_num][3]\n",
    "           \n",
    "            vector_1_y = prev_2_y - prev_1_y\n",
    "            vector_1_x = prev_2_x - prev_1_x\n",
    "            \n",
    "            vector_2_y = prev_1_y - location[0]\n",
    "            vector_2_x = prev_1_x - location[1]\n",
    "            \n",
    "            if(vector_1_y == 0 and vector_1_x == 0) or (vector_2_y == 0 and vector_2_x == 0):\n",
    "                #Didnt Move either this turn or last, therefore no direction to maintain\n",
    "                smoothness_reward = 0\n",
    "            elif(vector_1_y == vector_2_y) and (vector_1_x == vector_2_x):\n",
    "                # Vectors are the same therefore moved in the same direction\n",
    "                smoothness_reward = 1\n",
    "            else:\n",
    "                smoothness_reward = 0.1\n",
    "                \n",
    "            # Boundary - Reward Function\n",
    "            # Number of neighbours the location has\n",
    "            total_neighbours = 0\n",
    "            unvisited_neighbours = 0\n",
    "            action = 0\n",
    "            while action < 4:\n",
    "                temp_y, temp_x = location\n",
    "                temp_y += ACTIONS[action][0]\n",
    "                temp_x += ACTIONS[action][1]\n",
    "                if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                    total_neighbours += 1\n",
    "                    if not ((temp_y, temp_x) in visited_locations):\n",
    "                        unvisited_neighbours += 1\n",
    "                action += 1\n",
    "            \n",
    "            boundary_reward = (total_neighbours - unvisited_neighbours) / total_neighbours\n",
    "            \n",
    "            \n",
    "            # Combined Reward\n",
    "            reward = (predator_weight * predator_reward) + (smoothness_weight * smoothness_reward) + (boundary_weight * boundary_reward)\n",
    "            \n",
    "        else:\n",
    "            reward = -0.05\n",
    "        \n",
    "        # Edit prev locations\n",
    "        self.prev_robot_locations[robot_num][0] = self.prev_robot_locations[robot_num][2]\n",
    "        self.prev_robot_locations[robot_num][1] = self.prev_robot_locations[robot_num][3]\n",
    "        self.prev_robot_locations[robot_num][2] = location[0]\n",
    "        self.prev_robot_locations[robot_num][3] = location[1]\n",
    "        \n",
    "        return reward\n",
    "    def get_state_and_reward(self, state_history, robot_num, visited_locations):\n",
    "        return self.robot_positions[robot_num], self.give_reward(state_history, robot_num, visited_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7482fe",
   "metadata": {},
   "source": [
    "## Agent Class (Robots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2249e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class Agent(object):\n",
    "    def __init__(self, states, alpha=0.15, gamma = 0.9, random_factor=0.2, robot_num=9, planet_width=6):       \n",
    "        self.initial_alpha = alpha\n",
    "        self.alpha = alpha\n",
    "        self.learning_decay = 1\n",
    "        self.gamma = gamma\n",
    "        self.random_factor = random_factor\n",
    "        self.robot_num = robot_num\n",
    "        \n",
    "        self.state_history = [] #State, action \n",
    "        self.backtrack = -1 #Used for backtracking along path\n",
    "        self.can_backtrack = True\n",
    "        \n",
    "        # start the rewards table\n",
    "        self.qtable = np.zeros(((planet_width*planet_width), 4)).tolist()\n",
    "        \n",
    "        self.seen_locations = []\n",
    "\n",
    "    def update_state_history(self, state, action):\n",
    "        self.state_history.append((state, action))\n",
    "        \n",
    "    def reset_backtrack(self):\n",
    "        self.state_history = []\n",
    "        self.backtrack = -1\n",
    "        self.can_backtrack = True\n",
    "        \n",
    "    def learn(self, state, reward, action, next_state, episode, planet_width):\n",
    "        a = self.alpha\n",
    "        g = self.gamma\n",
    "        y, x = state\n",
    "        state_index = (y * planet_width) + x\n",
    "        current_qReward =  self.qtable[state_index][action]\n",
    "        Ny, Nx = next_state       \n",
    "        next_state_index = (Ny * planet_width) + Nx\n",
    "        self.qtable[state_index][action] = current_qReward + (a * (reward + (g * (max(self.qtable[next_state_index]))) - current_qReward))\n",
    "        self.random_factor = self.random_factor*0.9995 # decrease random_factor\n",
    "        if self.random_factor <= 0.001:\n",
    "            self.random_factor = 0.001 \n",
    "        self.alpha = (1/(1+self.learning_decay*episode))*self.initial_alpha\n",
    "            \n",
    "    def choose_action(self, state, allowed_moves, state_history, planet, visited_locations):        \n",
    "        next_move = None\n",
    "        n = np.random.uniform()\n",
    "        # Random\n",
    "        if self.backtrack == -1 and n < self.random_factor:\n",
    "            if not allowed_moves:\n",
    "                valid = False\n",
    "            else:\n",
    "                next_move = np.random.choice(allowed_moves)\n",
    "                valid = True\n",
    "        # Choose best action\n",
    "        else:\n",
    "            y, x = state\n",
    "            state_index = (y * planet.planet_width) + x\n",
    "            \n",
    "            unvisited_actions = []\n",
    "            \n",
    "            # Compile actions that vist unknown locations\n",
    "            action = 0\n",
    "            while action < 4:\n",
    "                temp_y, temp_x = state\n",
    "                temp_y += ACTIONS[action][0]\n",
    "                temp_x += ACTIONS[action][1]\n",
    "                if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                    if planet.planet[(temp_y,temp_x)] == 0: \n",
    "                        if not ((temp_y, temp_x) in visited_locations):\n",
    "                            action_reward = self.qtable[state_index][action]\n",
    "                            unvisited_actions.append((action_reward, action))\n",
    "                action += 1\n",
    "            \n",
    "            #If no unvisited neighbours back tracks\n",
    "            if unvisited_actions == []:\n",
    "                # Needs to take into account that backtracked state will be part of the history\n",
    "                # Check the list isnt over\n",
    "                if (len(state_history) >= -self.backtrack and self.can_backtrack == True):\n",
    "                    prev_entry = state_history[self.backtrack]\n",
    "                    prev_action = state_history[self.backtrack][1] # Need the action to move to this state\n",
    "                    self.backtrack -= 2 # Increment backtrack\n",
    "                    if prev_action == 0:\n",
    "                        next_move = 1\n",
    "                    elif prev_action == 1:\n",
    "                        next_move = 0\n",
    "                    elif prev_action == 2:\n",
    "                        next_move = 3\n",
    "                    elif prev_action == 3:\n",
    "                        next_move = 2\n",
    "                    valid = True\n",
    "                else:\n",
    "                    self.can_backtrack = False\n",
    "                    # Compile all possible actions\n",
    "                    action = 0\n",
    "                    while action < 4:\n",
    "                        temp_y, temp_x = state\n",
    "                        temp_y += ACTIONS[action][0]\n",
    "                        temp_x += ACTIONS[action][1]\n",
    "                        if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                            if planet.planet[(temp_y,temp_x)] == 0: \n",
    "                                action_reward = self.qtable[state_index][action]\n",
    "                                unvisited_actions.append((action_reward, action))\n",
    "                        action += 1                   \n",
    "                    # If still no possible moves\n",
    "                    if unvisited_actions == []:\n",
    "                        valid = False\n",
    "                    else:\n",
    "                        action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                        unvisited_actions.remove(action_label)\n",
    "                        next_move = action_label[1]    \n",
    "                        valid = True                       \n",
    "                \n",
    "            else:\n",
    "                self.backtrack = -1\n",
    "                self.can_backtrack = True\n",
    "                action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                unvisited_actions.remove(action_label)\n",
    "                next_move = action_label[1]    \n",
    "                valid = True\n",
    "                \n",
    "        return next_move, valid           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3128b",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8e3f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(object):\n",
    "    def __init__(self):\n",
    "        # Variables\n",
    "        self.fully_covered = False\n",
    "        \n",
    "        # Track visited locations\n",
    "        self.no_visited_locations = 0\n",
    "        self.visited_locations = []\n",
    "        \n",
    "    def reset_count(self):\n",
    "        # Reinitialise varaibles\n",
    "        self.fully_covered = False\n",
    "        self.no_visited_locations = 0\n",
    "        self.visited_locations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddda54",
   "metadata": {},
   "source": [
    "## Learning Loop - Augmented Q Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5a4f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':     \n",
    "    print(\"Hello\")\n",
    "    completed_count = 0\n",
    "    \n",
    "    planet = Planet(planet_width=planet_size, num_robots=num_robots)   \n",
    "    server = Server()\n",
    "    print(\"Intialised Planet\")\n",
    "    print(planet.planet)\n",
    "    \n",
    "    # Initialise Robots \n",
    "    swarm = []\n",
    "    for r in range(num_robots):\n",
    "        temp_robot = Agent(planet.planet, alpha=Learning_Rate, gamma=Error_Rate, random_factor=Epsilon, robot_num=r, planet_width=planet.planet_width)\n",
    "        swarm.append(temp_robot)\n",
    "    \n",
    "    moveHistory = []\n",
    "    \n",
    "    # Possible visitable locations\n",
    "    visitable_locations = (np.size(planet.planet) - np.count_nonzero(planet.planet == 1))\n",
    "    \n",
    "    for i in range(Episodes):\n",
    "        if i % 500 == 0:                    \n",
    "            print(\"Episode\",i)\n",
    "            print(\"Completed\", completed_count) \n",
    "            completed_count = 0\n",
    "        \n",
    "        #Intialise Variables\n",
    "        movementBudget = Movement_Budget\n",
    "        server.reset_count()\n",
    "        # vist all starting locations\n",
    "        for r in range(num_robots):\n",
    "            robot = swarm[r]\n",
    "            robot.reset_backtrack()\n",
    "            if planet.robot_positions[r] not in server.visited_locations:\n",
    "                server.visited_locations.append(planet.robot_positions[r])\n",
    "                server.no_visited_locations += 1\n",
    "        \n",
    "        while not server.fully_covered:  \n",
    "            for n in range(num_robots):\n",
    "                robot = swarm[n]\n",
    "                sHistory = robot.state_history\n",
    "                #state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the current state\n",
    "                state = planet.robot_positions[robot.robot_num]\n",
    "                action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory, planet, server.visited_locations) # choose an action (explore or exploit)\n",
    "                if valid:\n",
    "                    planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                    next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the new state and reward    \n",
    "                    robot.update_state_history(next_state, action)\n",
    "                    if next_state not in server.visited_locations:\n",
    "                        server.visited_locations.append(next_state)\n",
    "                        server.no_visited_locations += 1\n",
    "                        if server.no_visited_locations >= visitable_locations:\n",
    "                            completed_count += 1\n",
    "                            reward = 10\n",
    "                            \n",
    "                    robot.learn(state, reward, action, next_state, i, planet.planet_width)\n",
    "                    \n",
    "            planet.steps += 1\n",
    "            movementBudget -= 1        \n",
    "            if server.no_visited_locations >= visitable_locations: \n",
    "                server.fully_covered = True\n",
    "            elif movementBudget <= 0:\n",
    "                # End the episode if the movement budget runs out\n",
    "                server.fully_covered = True        \n",
    "\n",
    "        moveHistory.append(planet.steps) # get a history of number of steps taken to plot later\n",
    "        \n",
    "        # Intialise the variables\n",
    "        planet = Planet(planet_width=planet_size, num_robots=num_robots) \n",
    "        \n",
    "\n",
    "print(\"Episode\",i)\n",
    "print(\"Epsilon\",robot.random_factor)\n",
    "print(\"Completed\", completed_count) \n",
    "        \n",
    "print(\"Visited Locations:\", server.no_visited_locations, \"Steps:\", moveHistory[-1])  \n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"History of Steps\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number of Steps\")\n",
    "plt.semilogy(moveHistory, color=\"b\")\n",
    "\n",
    "#plt.semilogy(epsilonHistory, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0b541",
   "metadata": {},
   "source": [
    "# Normal Q Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b6842",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49011c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class QPlanet(object):\n",
    "    def __init__(self, planet_width, num_robots):\n",
    "        # start with defining your planet\n",
    "        self.planet_width = planet_width\n",
    "        self.planet_edge = planet_width - 1\n",
    "        self.planet = np.zeros((planet_width, planet_width)).astype(int)\n",
    "        \n",
    "        #Traps\n",
    "        \"\"\"\n",
    "        self.planet[2, 8] = 1\n",
    "        self.planet[4, 5] = 1\n",
    "        self.planet[4, 6] = 1\n",
    "        self.planet[4, 7] = 1\n",
    "        self.planet[5, 5] = 1\n",
    "        self.planet[5, 6] = 1\n",
    "        self.planet[5, 7] = 1\n",
    "        self.planet[9, 9] = 1\n",
    "        self.planet[9, 10] = 1\n",
    "        self.planet[10, 9] = 1\n",
    "        self.planet[10, 10] = 1\n",
    "        self.planet[7, 1] = 1\n",
    "        \"\"\"\n",
    "        #Robots\n",
    "        self.num_robots = num_robots\n",
    "        self.robot_positions = []\n",
    "        self.agent_setup()\n",
    "        \n",
    "        \n",
    "        self.steps = 0 # contains num steps robots took\n",
    "        self.allowed_states = None # for now, this is none\n",
    "        self.construct_allowed_states() \n",
    "    \n",
    "    def agent_setup(self):\n",
    "        for r in range(self.num_robots):\n",
    "            found = False\n",
    "            val_r = r\n",
    "            while found == False:  \n",
    "                temp_y = val_r // self.planet_width\n",
    "                temp_x = val_r % self.planet_width\n",
    "                if self.planet[temp_y, temp_x] == 0:\n",
    "                    found = True\n",
    "                else:\n",
    "                    val_r += 1\n",
    "                    \n",
    "            self.robot_positions.append((temp_y, temp_x))\n",
    "            self.planet[temp_y, temp_x] = 2\n",
    "            \n",
    "    \n",
    "    def is_allowed_move(self, state, action):\n",
    "        y, x = state\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        # moving off the board\n",
    "        if y < 0 or x < 0 or y > self.planet_edge or x > self.planet_edge:\n",
    "             return False\n",
    "        # moving into empty space\n",
    "        elif self.planet[y, x] != 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def construct_allowed_states(self):\n",
    "        allowed_states = {}\n",
    "        for y, row in enumerate(self.planet):\n",
    "            for x, col in enumerate(row):\n",
    "                # iterate through all valid spaces\n",
    "                if self.planet[(y,x)] != 1:\n",
    "                    allowed_states[(y,x)] = []\n",
    "                    for action in ACTIONS:\n",
    "                        if self.is_allowed_move((y, x), action):\n",
    "                            allowed_states[(y,x)].append(action)\n",
    "        self.allowed_states = allowed_states\n",
    "        \n",
    "    def update_planet(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        old_y, old_x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        self.planet[y, x] = 2\n",
    "        self.planet[old_y, old_x] = 0\n",
    "        self.robot_positions[robot_num] = (y, x)\n",
    "        \n",
    "    def invalid_move(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        return (y, x)  \n",
    "                \n",
    "    def is_game_over(fully_covered):\n",
    "        return fully_covered\n",
    "    \n",
    "    def give_reward(self, state_history, robot_num, visited_locations):\n",
    "        location = self.robot_positions[robot_num]\n",
    "        if location not in visited_locations:\n",
    "            return 1\n",
    "        else:\n",
    "            return -0.05\n",
    "    def get_state_and_reward(self, state_history, robot_num, visited_locations):\n",
    "        return self.robot_positions[robot_num], self.give_reward(state_history, robot_num, visited_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79402aad",
   "metadata": {},
   "source": [
    "## Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac71fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class QAgent(object):\n",
    "    def __init__(self, states, alpha=0.15, gamma = 0.9, random_factor=0.2, robot_num=9, planet_width=6):\n",
    "        self.state_history = [((0, 0), 0, 0, (0, 0))] # state, reward, action, next_state\n",
    "        self.initial_alpha = alpha\n",
    "        self.alpha = alpha\n",
    "        self.learning_decay = 1\n",
    "        self.gamma = gamma\n",
    "        self.random_factor = random_factor\n",
    "        self.robot_num = robot_num\n",
    "        \n",
    "        # start the rewards table\n",
    "        self.qtable = np.zeros(((planet_width*planet_width), 4)).tolist()\n",
    "        \n",
    "        self.seen_locations = []\n",
    "\n",
    "    def update_state_history(self, state, reward, action, next_state):\n",
    "        self.state_history.append((state, reward, action, next_state))\n",
    "        \n",
    "    def learn(self, state, reward, action, next_state, episode, planet_width):\n",
    "        a = self.alpha\n",
    "        g = self.gamma\n",
    "        y, x = state\n",
    "        state_index = (y * planet_width) + x\n",
    "        current_qReward =  self.qtable[state_index][action]\n",
    "        Ny, Nx = next_state       \n",
    "        next_state_index = (Ny * planet_width) + Nx\n",
    "        self.qtable[state_index][action] = current_qReward + (a * (reward + (g * (max(self.qtable[next_state_index]))) - current_qReward))\n",
    "        self.state_history = [] # reset the state_history\n",
    "        self.random_factor = self.random_factor*0.9995 # decrease random_factor\n",
    "        if self.random_factor <= 0.001:\n",
    "            self.random_factor = 0.001 \n",
    "        self.alpha = (1/(1+self.learning_decay*episode))*self.initial_alpha\n",
    "            \n",
    "    def choose_action(self, state, allowed_moves, state_history, planet, visited_locations):        \n",
    "        next_move = None\n",
    "        n = np.random.uniform()\n",
    "        # Random\n",
    "        if n < self.random_factor:\n",
    "            if not allowed_moves:\n",
    "                valid = False\n",
    "            else:\n",
    "                next_move = np.random.choice(allowed_moves)\n",
    "                valid = True\n",
    "        # Choose best action\n",
    "        else:\n",
    "            y, x = state\n",
    "            state_index = (y * planet.planet_width) + x\n",
    "            \n",
    "            unvisited_actions = []\n",
    "            \n",
    "            # Compile actions that vist unknown locations\n",
    "            action = 0\n",
    "            while action < 4:\n",
    "                temp_y, temp_x = state\n",
    "                temp_y += ACTIONS[action][0]\n",
    "                temp_x += ACTIONS[action][1]\n",
    "                if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                    if planet.planet[(temp_y,temp_x)] == 0: \n",
    "                        if not ((temp_y, temp_x) in visited_locations):\n",
    "                            action_reward = self.qtable[state_index][action]\n",
    "                            unvisited_actions.append((action_reward, action))\n",
    "                action += 1\n",
    "            \n",
    "            #If no unvisited locations looks at all possibilites\n",
    "            if unvisited_actions == []:\n",
    "                # Compile all possible actions\n",
    "                action = 0\n",
    "                while action < 4:\n",
    "                    temp_y, temp_x = state\n",
    "                    temp_y += ACTIONS[action][0]\n",
    "                    temp_x += ACTIONS[action][1]\n",
    "                    if not (temp_y < 0 or temp_x < 0 or temp_y > planet.planet_edge or temp_x > planet.planet_edge):\n",
    "                        if planet.planet[(temp_y,temp_x)] == 0: \n",
    "                            action_reward = self.qtable[state_index][action]\n",
    "                            unvisited_actions.append((action_reward, action))\n",
    "                    action += 1                   \n",
    "                # If still no possible moves\n",
    "                if unvisited_actions == []:\n",
    "                    valid = False\n",
    "                else:\n",
    "                    action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                    unvisited_actions.remove(action_label)\n",
    "                    next_move = action_label[1]    \n",
    "                    valid = True                       \n",
    "            else:\n",
    "                action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                unvisited_actions.remove(action_label)\n",
    "                next_move = action_label[1]    \n",
    "                valid = True\n",
    "        return next_move, valid           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f104c",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d79238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QServer(object):\n",
    "    def __init__(self):\n",
    "        # Variables\n",
    "        self.fully_covered = False\n",
    "        \n",
    "        # Track visited locations\n",
    "        self.no_visited_locations = 0\n",
    "        self.visited_locations = []\n",
    "        \n",
    "    def reset_count(self):\n",
    "        # Reinitialise varaibles\n",
    "        self.fully_covered = False\n",
    "        self.no_visited_locations = 0\n",
    "        self.visited_locations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9991973",
   "metadata": {},
   "source": [
    "## Learning Loop of Q Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':     \n",
    "    print(\"Hello\")\n",
    "    completed_count = 0\n",
    "    \n",
    "    planet = QPlanet(planet_width=planet_size, num_robots=num_robots)   \n",
    "    server = QServer()\n",
    "    print(\"Intialised Planet\")\n",
    "    print(planet.planet)\n",
    "    \n",
    "    # Initialise Robots \n",
    "    Qswarm = []\n",
    "    for r in range(num_robots):\n",
    "        temp_robot = QAgent(planet.planet, alpha=Learning_Rate, gamma=Error_Rate, random_factor=Epsilon, robot_num=r, planet_width=planet.planet_width)\n",
    "        Qswarm.append(temp_robot)\n",
    "    \n",
    "    moveHistory = []\n",
    "    epsilonHistory = []\n",
    "    \n",
    "    # Possible visitable locations\n",
    "    visitable_locations = (np.size(planet.planet) - np.count_nonzero(planet.planet == 1))\n",
    "    \n",
    "    for i in range(Episodes):\n",
    "        if i % 500 == 0:                    \n",
    "            print(\"Episode\",i)\n",
    "            print(\"Completed\", completed_count) \n",
    "            completed_count = 0\n",
    "        \n",
    "        #Intialise Variables\n",
    "        movementBudget = Movement_Budget\n",
    "        server.reset_count()\n",
    "        # vist all starting locations\n",
    "        for r in range(num_robots):\n",
    "            if planet.robot_positions[r] not in server.visited_locations:\n",
    "                server.visited_locations.append(planet.robot_positions[r])\n",
    "                server.no_visited_locations += 1\n",
    "        \n",
    "        while not server.fully_covered:  \n",
    "            for n in range(num_robots):\n",
    "                robot = Qswarm[n]\n",
    "                sHistory = robot.state_history\n",
    "                state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the current state\n",
    "                action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory, planet, server.visited_locations) # choose an action (explore or exploit)\n",
    "                # There is a possible move for the robot\n",
    "                if valid:\n",
    "                    planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                    next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the new state and reward    \n",
    "                    if next_state not in server.visited_locations:\n",
    "                        server.visited_locations.append(next_state)\n",
    "                        server.no_visited_locations += 1\n",
    "                        if server.no_visited_locations >= visitable_locations:\n",
    "                            completed_count += 1\n",
    "                            reward = 10\n",
    "                    robot.learn(state, reward, action, next_state, i, planet.planet_width)\n",
    "                \n",
    "            planet.steps += 1\n",
    "            movementBudget -= 1\n",
    "            if server.no_visited_locations >= visitable_locations: \n",
    "                server.fully_covered = True\n",
    "            elif movementBudget <= 0:\n",
    "                # End the episode if the movement budget runs out\n",
    "                server.fully_covered = True        \n",
    "\n",
    "        moveHistory.append(planet.steps) # get a history of number of steps taken to plot later\n",
    "        epsilonHistory.append(robot.random_factor)\n",
    "        \n",
    "        # Intialise the variables\n",
    "        planet = QPlanet(planet_width=planet_size, num_robots=num_robots) \n",
    "        \n",
    "\n",
    "print(\"Episode\",i)\n",
    "print(\"Epsilon\",robot.random_factor)\n",
    "print(\"Completed\", completed_count) \n",
    "        \n",
    "print(\"Visited Locations:\", server.no_visited_locations, \"Steps:\", moveHistory[-1])  \n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.semilogy(moveHistory, color=\"b\")\n",
    "#plt.semilogy(epsilonHistory, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b5325",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd8064",
   "metadata": {},
   "source": [
    "## Testing of Augmented Q Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5858623",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = plt.cm.jet(np.linspace(0,1,simulationLoops))\n",
    "\n",
    "totalCoverData = []\n",
    "totalMovementData = []\n",
    "\n",
    "for l in range(simulationLoops):\n",
    "    planet = Planet(planet_width=planet_size, num_robots=num_robots)\n",
    "    server.reset_count()\n",
    "    pathData = []\n",
    "\n",
    "    vist_count = 0\n",
    "    action_history = []\n",
    "    #Intialise Variables\n",
    "    movementBudget = Movement_Budget\n",
    "    server.reset_count()\n",
    "    #Graph data\n",
    "    coverData = []\n",
    "    movementData = []\n",
    "    dataLength = Movement_Budget\n",
    "\n",
    "    # vist all starting locations\n",
    "    for r in range(num_robots):\n",
    "        robot = swarm[r]\n",
    "        robot.reset_backtrack()\n",
    "        if planet.robot_positions[r] not in server.visited_locations:\n",
    "            server.visited_locations.append(planet.robot_positions[r])\n",
    "            server.no_visited_locations += 1\n",
    "\n",
    "    coverData.append((server.no_visited_locations/visitable_locations)*100)\n",
    "    movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "\n",
    "    while not server.fully_covered:  \n",
    "        for n in range(num_robots):\n",
    "            robot = swarm[n]\n",
    "            sHistory = robot.state_history\n",
    "            #state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the current state\n",
    "            state = planet.robot_positions[robot.robot_num]\n",
    "            action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory, planet, server.visited_locations) # choose an action (explore or exploit)\n",
    "            if valid:\n",
    "                planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the new state and reward    \n",
    "                robot.update_state_history(next_state, action)\n",
    "                if next_state not in server.visited_locations:\n",
    "                    server.visited_locations.append(next_state)\n",
    "                    server.no_visited_locations += 1\n",
    "                    if server.no_visited_locations >= visitable_locations:\n",
    "                        completed_count += 1\n",
    "                        reward = 10\n",
    "                robot.learn(state, reward, action, next_state, i, planet.planet_width)\n",
    "\n",
    "        planet.steps += 1\n",
    "        movementBudget -= 1\n",
    "        coverData.append((server.no_visited_locations/visitable_locations)*100)\n",
    "        movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "\n",
    "        if server.no_visited_locations >= visitable_locations: \n",
    "            server.fully_covered = True\n",
    "        elif movementBudget <= 0:\n",
    "            # End the episode if the movement budget runs out\n",
    "            server.fully_covered = True       \n",
    "\n",
    "    # Fill the arrays to the complete size \n",
    "    while len(coverData) <= dataLength:\n",
    "        coverData.append(100)\n",
    "\n",
    "    while len(movementData) <= dataLength:\n",
    "        movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "    \n",
    "    # Add this loops data to hisotry for total plotting\n",
    "    totalCoverData.append(coverData)\n",
    "    totalMovementData.append(movementData)\n",
    "\n",
    "# Complete Graphs\n",
    "# cover%/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Covered/Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"% Covered\")\n",
    "for x in range(simulationLoops):\n",
    "    plt.plot(totalCoverData[x], color=\"b\")\n",
    "plt.ylim(0,105)\n",
    "\n",
    "# budget/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Movement/Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"% of Movement Budget Expended\")\n",
    "for x in range(simulationLoops):\n",
    "    plt.plot(totalMovementData[x], color=\"b\")\n",
    "plt.ylim(0,105)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a120522",
   "metadata": {},
   "source": [
    "## Testing of Q Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.jet(np.linspace(0,1,simulationLoops))\n",
    "\n",
    "totalCoverDataQ = []\n",
    "totalMovementDataQ = []\n",
    "\n",
    "for l in range(simulationLoops):\n",
    "    print(l)\n",
    "    planet = QPlanet(planet_width=planet_size, num_robots=num_robots)\n",
    "    server.reset_count()\n",
    "    pathData = []\n",
    "\n",
    "    vist_count = 0\n",
    "    action_history = []\n",
    "    #Intialise Variables\n",
    "    movementBudget = Movement_Budget\n",
    "    server.reset_count()\n",
    "    #Graph data\n",
    "    coverData = []\n",
    "    movementData = []\n",
    "    dataLength = Movement_Budget\n",
    "\n",
    "    # vist all starting locations\n",
    "    for r in range(num_robots):\n",
    "        robot = Qswarm[r]\n",
    "        if planet.robot_positions[r] not in server.visited_locations:\n",
    "            server.visited_locations.append(planet.robot_positions[r])\n",
    "            server.no_visited_locations += 1\n",
    "\n",
    "    coverData.append((server.no_visited_locations/visitable_locations)*100)\n",
    "    movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "\n",
    "    while not server.fully_covered  and movementBudget >= 0:  \n",
    "        for n in range(num_robots):\n",
    "            robot = Qswarm[n]\n",
    "            sHistory = robot.state_history\n",
    "            state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the current state \n",
    "            action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory, planet, server.visited_locations) # choose an action (explore or exploit)\n",
    "            # There is a possible move for the robot\n",
    "            if valid:\n",
    "                planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, server.visited_locations) # get the new state and reward    \n",
    "                if next_state not in server.visited_locations:\n",
    "                    server.visited_locations.append(next_state)\n",
    "                    server.no_visited_locations += 1\n",
    "                    if server.no_visited_locations >= visitable_locations:\n",
    "                        completed_count += 1\n",
    "                        reward = 10\n",
    "                robot.learn(state, reward, action, next_state, i, planet.planet_width)\n",
    "\n",
    "        planet.steps += 1\n",
    "        movementBudget -= 1\n",
    "        coverData.append((server.no_visited_locations/visitable_locations)*100)\n",
    "        movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "        if server.no_visited_locations >= visitable_locations: \n",
    "            server.fully_covered = True\n",
    "        elif movementBudget <= 0:\n",
    "            # End the episode if the movement budget runs out\n",
    "            server.fully_covered = True       \n",
    "\n",
    "    # Fill the arrays to the complete size \n",
    "    while len(coverData) <= dataLength:\n",
    "        coverData.append(100)\n",
    "\n",
    "    while len(movementData) <= dataLength:\n",
    "        movementData.append(((Movement_Budget - movementBudget)/Movement_Budget)*100)\n",
    "    \n",
    "    # Add this loops data to hisotry for total plotting\n",
    "    totalCoverDataQ.append(coverData)\n",
    "    totalMovementDataQ.append(movementData)\n",
    "\n",
    "# Complete Graphs\n",
    "# cover%/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Covered/Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"% Covered\")\n",
    "for x in range(simulationLoops):\n",
    "    plt.plot(totalCoverDataQ[x], color=\"g\")\n",
    "plt.ylim(0,105)\n",
    "\n",
    "# budget/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Movement/Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"% of Movement Budget Expended\")\n",
    "for x in range(simulationLoops):\n",
    "    plt.plot(totalMovementDataQ[x], color=\"g\")\n",
    "plt.ylim(0,105)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412de00",
   "metadata": {},
   "source": [
    "# Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2492b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combined Graphs\n",
    "# cover%/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Covered/Time\", fontsize=28)\n",
    "plt.xlabel(\"Time\", fontsize=28)\n",
    "plt.ylabel(\"% Covered\", fontsize=28)\n",
    "for x in range(simulationLoops - 1):\n",
    "    plt.plot(totalCoverDataQ[x], color=\"g\")\n",
    "    plt.plot(totalCoverData[x], color=\"b\")\n",
    "plt.plot(totalCoverDataQ[simulationLoops - 1], color=\"g\", label=\"Standard Q-Learning\")\n",
    "plt.plot(totalCoverData[simulationLoops - 1], color=\"b\", label=\"Pred-Prey Q-Learning\")\n",
    "plt.ylim(0,105)  \n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "\n",
    "# budget/t\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Movement/Time\", fontsize=28)\n",
    "plt.xlabel(\"Time\", fontsize=28)\n",
    "plt.ylabel(\"% of Movement Budget Expended\", fontsize=28)\n",
    "for x in range(simulationLoops - 1):\n",
    "    plt.plot(totalMovementDataQ[x], color=\"g\")\n",
    "    plt.plot(totalMovementData[x], color=\"b\")\n",
    "plt.plot(totalMovementDataQ[simulationLoops - 1], color=\"g\", label=\"Standard Q-Learning\")\n",
    "plt.plot(totalMovementData[simulationLoops - 1], color=\"b\", label=\"Pred-Prey Q-Learning\")\n",
    "plt.ylim(0,105)  \n",
    "plt.legend(loc=\"upper left\", fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
