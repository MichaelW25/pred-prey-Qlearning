{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc3fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb4136",
   "metadata": {},
   "source": [
    "## Environments (Planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d43e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class Planet(object):\n",
    "    def __init__(self):\n",
    "        # start with defining your planet       \n",
    "        self.planet = np.zeros((6, 6)).astype(int)\n",
    "        #POIs\n",
    "        #self.planet[5, 5] = 3\n",
    "        #self.planet[2, 1] = 3\n",
    "        #self.planet[3, 4] = 3\n",
    "        #Robots\n",
    "        self.planet[0, 0] = 2\n",
    "        #Traps\n",
    "        #self.planet[3, 4] = 1\n",
    "        #self.planet[4, 2] = 1\n",
    "        #self.planet[10, 2] = 1\n",
    "        #self.planet[7, 0] = 1        \n",
    "        #self.planet[7, 6] = 1\n",
    "        #self.planet[14, 12] = 1\n",
    "        #self.planet[11, 5] = 1\n",
    "        #self.planet[5, 8] = 1        \n",
    "        #self.planet[2:7, 12] = 1\n",
    "        \n",
    "        self.robot_positions = [(0,0)]\n",
    "        self.robot_0_done = False\n",
    "        self.steps = 0 # contains num steps robot took\n",
    "        self.allowed_states = None # for now, this is none\n",
    "        self.construct_allowed_states() \n",
    "        \n",
    "    def is_allowed_move(self, state, action):\n",
    "        y, x = state\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        # moving off the board\n",
    "        if y < 0 or x < 0 or y > 5 or x > 5:\n",
    "             return False\n",
    "        # moving into empty space or POI\n",
    "        # if self.planet[y, x] == 0 or self.planet[y, x] == 3:\n",
    "            #return True\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def construct_allowed_states(self):\n",
    "        allowed_states = {}\n",
    "        for y, row in enumerate(self.planet):\n",
    "            for x, col in enumerate(row):\n",
    "                # iterate through all valid spaces\n",
    "                if self.planet[(y,x)] != 1:\n",
    "                    allowed_states[(y,x)] = []\n",
    "                    for action in ACTIONS:\n",
    "                        if self.is_allowed_move((y, x), action):\n",
    "                            allowed_states[(y,x)].append(action)\n",
    "        self.allowed_states = allowed_states\n",
    "        \n",
    "    def update_planet(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        old_y, old_x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        #if self.planet[y, x] != 2:\n",
    "        self.planet[y, x] = 2\n",
    "        self.planet[old_y, old_x] = 0\n",
    "        self.robot_positions[robot_num] = (y, x)\n",
    "        \n",
    "    def invalid_move(self, action, robot_num):\n",
    "        y, x = self.robot_positions[robot_num]\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        return (y, x)\n",
    "        \n",
    "                \n",
    "    def is_game_over(self):\n",
    "        if self.robot_0_done == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def give_reward(self, state_history, robot_num, visited_locations):\n",
    "        location = self.robot_positions[robot_num]\n",
    "        if location not in visited_locations:\n",
    "            return 1\n",
    "        else:\n",
    "            return -0.05\n",
    "    def get_state_and_reward(self, state_history, robot_num, visited_locations):\n",
    "        return self.robot_positions[robot_num], self.give_reward(state_history, robot_num, visited_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7482fe",
   "metadata": {},
   "source": [
    "## Agent Class (Robots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2249e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U D L R\n",
    "ACTIONS = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "class Agent(object):\n",
    "    def __init__(self, states, alpha=0.15, gamma = 0.9, random_factor=0.2, robot_num=9):\n",
    "        self.state_history = [((0, 0), 0, 0, (0, 0))] # state, reward, action, next_state\n",
    "        self.initial_alpha = alpha\n",
    "        self.alpha = alpha\n",
    "        self.learning_decay = 1\n",
    "        self.gamma = gamma\n",
    "        self.random_factor = random_factor\n",
    "        self.robot_num = robot_num\n",
    "        \n",
    "        # start the rewards table\n",
    "        self.qtable = np.zeros((36, 4)).tolist()\n",
    "        \n",
    "        self.visited_locations = [(0,0)]\n",
    "        self.no_visited_locations = 0\n",
    "        self.seen_locations = []\n",
    "\n",
    "    def update_state_history(self, state, reward, action, next_state):\n",
    "        self.state_history.append((state, reward, action, next_state))\n",
    "        \n",
    "    def trapped(self, reward):\n",
    "        temp = (self.state_history.pop())\n",
    "        list_s_history = list(temp)\n",
    "        list_s_history[1] = reward\n",
    "        tup_n_history = tuple(list_s_history)\n",
    "        self.state_history.append(tup_n_history)\n",
    "        \n",
    "    def reset_count(self):\n",
    "        self.visited_locations = []\n",
    "        self.no_visited_locations = 0\n",
    "        \n",
    "    def learn(self, state, reward, action, next_state, episode):\n",
    "        a = self.alpha\n",
    "        g = self.gamma\n",
    "        y, x = state\n",
    "        state_index = (y * 6) + x\n",
    "        current_qReward =  self.qtable[state_index][action]\n",
    "        Ny, Nx = next_state       \n",
    "        next_state_index = (Ny * 5) + Nx\n",
    "        #self.qtable[state_index][action] = reward + (g * (max(self.qtable[next_state_index])))\n",
    "        self.qtable[state_index][action] = current_qReward + (a * (reward + (g * (max(self.qtable[next_state_index]))) - current_qReward))\n",
    "        self.state_history = [] # reset the state_history\n",
    "        self.random_factor = self.random_factor*0.9995 # decrease random_factor\n",
    "        if self.random_factor <= 0.01:\n",
    "            self.random_factor = 0.01 \n",
    "        self.alpha = (1/(1+self.learning_decay*episode))*self.initial_alpha\n",
    "            \n",
    "    def choose_action(self, state, allowed_moves, state_history):\n",
    "        next_move = None\n",
    "        n = np.random.uniform()\n",
    "        if n < self.random_factor:\n",
    "            next_move = np.random.choice(allowed_moves)\n",
    "            valid = True\n",
    "        else:\n",
    "            y, x = state\n",
    "            state_index = (y * 6) + x\n",
    "            \n",
    "            unvisited_actions = []\n",
    "            \n",
    "            action = 0\n",
    "            while action < 4:\n",
    "                temp_y, temp_x = state\n",
    "                temp_y += ACTIONS[action][0]\n",
    "                temp_x += ACTIONS[action][1]\n",
    "                if not (temp_y < 0 or temp_x < 0 or temp_y > 5 or temp_x > 5):\n",
    "                    if not ((temp_y, temp_x) in self.visited_locations):\n",
    "                        action_reward = self.qtable[state_index][action]\n",
    "                        unvisited_actions.append((action_reward, action))\n",
    "                action += 1\n",
    "            \n",
    "            if unvisited_actions == []:\n",
    "                action = 0\n",
    "                while action < 4:\n",
    "                    temp_y, temp_x = state\n",
    "                    temp_y += ACTIONS[action][0]\n",
    "                    temp_x += ACTIONS[action][1]\n",
    "                    if not (temp_y < 0 or temp_x < 0 or temp_y > 5 or temp_x > 5):\n",
    "                        action_reward = self.qtable[state_index][action]\n",
    "                        unvisited_actions.append((action_reward, action))\n",
    "                    action += 1\n",
    "                action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                unvisited_actions.remove(action_label)\n",
    "                next_move = action_label[1]    \n",
    "                valid = True\n",
    "            else:\n",
    "                action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "                unvisited_actions.remove(action_label)\n",
    "                next_move = action_label[1]    \n",
    "                valid = True\n",
    "                # Get univisited possiblilities:\n",
    "        return next_move, valid  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddda54",
   "metadata": {},
   "source": [
    "## Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5a4f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  \n",
    "    #Hyper Parameters\n",
    "    Learning_Rate = 0.001\n",
    "    Error_Rate = 0.9\n",
    "    Epsilon = 0.5\n",
    "    Episodes = 4000\n",
    "    Movement_Budget = 500\n",
    "    completed_count = 0\n",
    "    \n",
    "    planet = Planet()\n",
    "    robot = Agent(planet.planet, alpha=Learning_Rate, gamma=Error_Rate, random_factor=Epsilon, robot_num=0)\n",
    "    moveHistory = []\n",
    "    epsilonHistory = []\n",
    "    \n",
    "    for i in range(Episodes):\n",
    "        if i % 1000 == 0:                    \n",
    "            print(\"Episode\",i)\n",
    "            print(\"Epsilon\",robot.random_factor)\n",
    "            print(\"Completed\", completed_count) \n",
    "            completed_count = 0\n",
    "        movementBudget = Movement_Budget\n",
    "        robot.reset_count()\n",
    "        \n",
    "        while not planet.is_game_over():\n",
    "            sHistory = robot.state_history\n",
    "            state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, robot.visited_locations) # get the current state\n",
    "            action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory) # choose an action (explore or exploit)\n",
    "            if valid:\n",
    "                planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, robot.visited_locations) # get the new state and reward    \n",
    "                if next_state not in robot.visited_locations:\n",
    "                    robot.visited_locations.append(next_state)\n",
    "                    robot.no_visited_locations += 1\n",
    "                    if robot.no_visited_locations >=36:\n",
    "                        completed_count += 1\n",
    "                        reward = 10\n",
    "                    if next_state in robot.seen_locations:\n",
    "                        robot.seen_locations.remove(next_state)\n",
    "\n",
    "                robot.learn(state, reward, action, next_state, i)\n",
    "                #robot.update_state_history(state, reward, action, next_state) # update the robot memory with state and reward\n",
    "            \n",
    "            planet.steps += 1\n",
    "            movementBudget -= 1\n",
    "            if robot.no_visited_locations >= 36: \n",
    "                planet.robot_0_done = True\n",
    "            elif movementBudget <= 0:\n",
    "                planet.robot_0_done = True        \n",
    "        \n",
    "        # robot should learn after every episode\n",
    "\n",
    "        moveHistory.append(planet.steps) # get a history of number of steps taken to plot later\n",
    "        epsilonHistory.append(robot.random_factor)\n",
    "        planet = Planet() # reinitialize the planet\n",
    "\n",
    "print(\"Episode\",i)\n",
    "print(\"Epsilon\",robot.random_factor)\n",
    "print(\"Completed\", completed_count) \n",
    "        \n",
    "print(\"Visited Locations:\", robot.no_visited_locations, \"Steps:\", moveHistory[-1])  \n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.semilogy(moveHistory, color=\"b\")\n",
    "#plt.semilogy(epsilonHistory, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd8064",
   "metadata": {},
   "source": [
    "## Testing and Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d2f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testHistory = []\n",
    "\n",
    "robot.reset_count()\n",
    "\n",
    "for i in range(1):\n",
    "        if i % 100 == 0:            \n",
    "            print(i)\n",
    "        \n",
    "        planet = Planet()\n",
    "        tx, ty, px, py, rx, ry, x_history, y_history = [], [], [], [], [], [], [], []\n",
    "        \n",
    "        for y, row in enumerate(planet.planet):\n",
    "            for x, col in enumerate(row):\n",
    "                if planet.planet[(y,x)] == 1:\n",
    "                    tx.append(x)\n",
    "                    ty.append(y)\n",
    "                if planet.planet[(y,x)] == 3:\n",
    "                    px.append(x)\n",
    "                    py.append(y)\n",
    "                if planet.planet[(y,x)] == 2:\n",
    "                    rx.append(x)\n",
    "                    ry.append(y)\n",
    "        x_history.append(rx)\n",
    "        y_history.append(ry)\n",
    "        plt.scatter(rx, ry, marker = 'o', color = 'g') \n",
    "        plt.scatter(tx, ty, marker = 'x', color = 'r') \n",
    "        plt.scatter(px, py, marker = '*', color = 'y') \n",
    "        plt.xlim([-1, 6])\n",
    "        plt.ylim([-1, 6])   \n",
    "        plt.show()\n",
    "        \n",
    "        movementBudget = Movement_Budget\n",
    "        robot.reset_count()\n",
    "        robot.random_factor = 0\n",
    "        vist_count = 0\n",
    "        count = 0\n",
    "        action_history = []\n",
    "        \n",
    "        state = (0,0)\n",
    "        \n",
    "        \"\"\"\n",
    "        while vist_count < 36 and count < 600:\n",
    "            count += 1\n",
    "            y, x = state\n",
    "            Sindex = (y * 6) + x\n",
    "            state_index = (y * 6) + x\n",
    "            \n",
    "            unvisited_actions = []\n",
    "            action = 0\n",
    "            while action < 4:\n",
    "                temp_y, temp_x = state\n",
    "                temp_y += ACTIONS[action][0]\n",
    "                temp_x += ACTIONS[action][1]\n",
    "                if not (temp_y < 0 or temp_x < 0 or temp_y > 5 or temp_x > 5):\n",
    "                    action_reward = robot.qtable[state_index][action]\n",
    "                    unvisited_actions.append((action_reward, action))\n",
    "                action += 1\n",
    "                       \n",
    "            action_label = unvisited_actions[unvisited_actions.index(max(unvisited_actions))]\n",
    "            next_move = action_label[1]    \n",
    "\n",
    "            action_history.append(next_move)\n",
    "            planet.update_planet(next_move, robot.robot_num)\n",
    "            state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, robot.visited_locations)\n",
    "            if state not in robot.visited_locations:\n",
    "                robot.visited_locations.append(state)\n",
    "                vist_count += 1                                  \n",
    "            \"\"\"\n",
    "        \n",
    "        while not planet.is_game_over():\n",
    "            sHistory = robot.state_history\n",
    "            state, _ = planet.get_state_and_reward(sHistory, robot.robot_num, robot.visited_locations) # get the current state\n",
    "            action, valid = robot.choose_action(state, planet.allowed_states[state], sHistory) # choose an action (explore or exploit)\n",
    "            if valid:\n",
    "                planet.update_planet(action, robot.robot_num) # update the planet according to the action\n",
    "                next_state, reward = planet.get_state_and_reward(sHistory, robot.robot_num, robot.visited_locations) # get the new state and reward    \n",
    "                if next_state not in robot.visited_locations:\n",
    "                    robot.visited_locations.append(next_state)\n",
    "                    robot.no_visited_locations += 1\n",
    "                    if robot.no_visited_locations >=36:\n",
    "                        completed_count += 1\n",
    "                        reward = 10\n",
    "                    if next_state in robot.seen_locations:\n",
    "                        robot.seen_locations.remove(next_state)\n",
    "\n",
    "                robot.learn(state, reward, action, next_state, i)\n",
    "                #robot.update_state_history(state, reward, action, next_state) # update the robot memory with state and reward\n",
    "            \n",
    "            planet.steps += 1\n",
    "            movementBudget -= 1\n",
    "            if robot.no_visited_locations >= 36: \n",
    "                planet.robot_0_done = True\n",
    "            elif movementBudget <= 0:\n",
    "                planet.robot_0_done = True\n",
    "\n",
    "            rx = []\n",
    "            ry = []\n",
    "            for y, row in enumerate(planet.planet):\n",
    "                for x, col in enumerate(row):\n",
    "                    if planet.planet[(y,x)] == 2:\n",
    "                        rx.append(x)\n",
    "                        ry.append(y)\n",
    "            x_history.append(rx)\n",
    "            y_history.append(ry)\n",
    "        \n",
    "        print(\"Count\", count)\n",
    "        testHistory.append(planet.steps) # get a history of number of steps taken to plot later\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "                  \n",
    "        def animate(i):\n",
    "            if i % 1 == 0:            \n",
    "                print(i) \n",
    "            plt.clf()\n",
    "            plt.xlim([-1, 6])\n",
    "            plt.ylim([-1, 6])\n",
    "            plt.scatter(x_history[i], y_history[i], marker = 'o', color = 'g') \n",
    "            px.append(x_history[i])\n",
    "            py.append(y_history[i])\n",
    "            plt.scatter(px, py, marker = '*', color = 'y') \n",
    "            return fig,\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, animate, repeat=True,\n",
    "                                            frames=len(x_history) - 1, interval=500)        \n",
    "        # To save the animation using Pillow as a gif\n",
    "        writer = animation.PillowWriter(fps=5,\n",
    "                                     metadata=dict(artist='Me'),\n",
    "                                     bitrate=1800)\n",
    "        ani.save('path.gif', writer=writer)\n",
    "        plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
